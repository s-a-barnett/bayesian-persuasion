
@article{khani_planning_2018,
	title = {Planning, Inference and Pragmatics in Sequential Language Games},
	volume = {6},
	issn = {2307-387X},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00037},
	doi = {10.1162/tacl_a_00037},
	abstract = {We study sequential language games in which two players, each with private information, communicate to achieve a common goal. In such games, a successful player must (i) infer the partner’s private information from the partner’s messages, (ii) generate messages that are most likely to help with the goal, and (iii) reason pragmatically about the partner’s strategy. We propose a model that captures all three characteristics and demonstrate their importance in capturing human behavior on a new goal-oriented dataset we collected using crowdsourcing.},
	pages = {543--555},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Khani, Fereshte and Goodman, Noah D. and Liang, Percy},
	urldate = {2019-10-10},
	date = {2018-12},
	langid = {english},
	file = {Khani et al. - 2018 - Planning, Inference and Pragmatics in Sequential L.pdf:/Users/sambarnett/Zotero/storage/ZRFMG5DW/Khani et al. - 2018 - Planning, Inference and Pragmatics in Sequential L.pdf:application/pdf}
}

@article{hilgard_learning_2019,
	title = {Learning Representations by Humans, for Humans},
	url = {http://arxiv.org/abs/1905.12686},
	abstract = {We propose a new, complementary approach to interpretability, in which machines are not considered as experts whose role it is to suggest what should be done and why, but rather as advisers. The objective of these models is to communicate to a human decision-maker not what to decide but how to decide. In this way, we propose that machine learning pipelines will be more readily adopted, since they allow a decision-maker to retain agency. Speciﬁcally, we develop a framework for learning representations by humans, for humans, in which we learn representations of inputs (‘advice’) that are effective for human decision-making. Representationgenerating models are trained with humans-in-the-loop, implicitly incorporating the human decision-making model. We show that optimizing for human decisionmaking rather than accuracy is effective in promoting good decisions in various classiﬁcation tasks while inherently maintaining a sense of interpretability.},
	journaltitle = {{arXiv}:1905.12686 [cs, stat]},
	author = {Hilgard, Sophie and Rosenfeld, Nir and Banaji, Mahzarin R. and Cao, Jack and Parkes, David C.},
	urldate = {2019-10-10},
	date = {2019-05-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.12686},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Human-Computer Interaction},
	file = {Hilgard et al. - 2019 - Learning Representations by Humans, for Humans.pdf:/Users/sambarnett/Zotero/storage/SQ4GUTWE/Hilgard et al. - 2019 - Learning Representations by Humans, for Humans.pdf:application/pdf}
}

@article{shafto_rational_2014,
	title = {A rational account of pedagogical reasoning: Teaching by, and learning from, examples},
	volume = {71},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028514000024},
	doi = {10.1016/j.cogpsych.2013.12.004},
	shorttitle = {A rational account of pedagogical reasoning},
	abstract = {Much of learning and reasoning occurs in pedagogical situations—situations in which a person who knows a concept chooses examples for the purpose of helping a learner acquire the concept. We introduce a model of teaching and learning in pedagogical settings that predicts which examples teachers should choose and what learners should infer given a teacher’s examples. We present three experiments testing the model predictions for rule-based, prototype, and causally structured concepts. The model shows good quantitative and qualitative ﬁts to the data across all three experiments, predicting novel qualitative phenomena in each case. We conclude by discussing implications for understanding concept learning and implications for theoretical claims about the role of pedagogy in human learning.},
	pages = {55--89},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Shafto, Patrick and Goodman, Noah D. and Griffiths, Thomas L.},
	urldate = {2019-10-14},
	date = {2014-06},
	langid = {english},
	file = {Shafto et al. - 2014 - A rational account of pedagogical reasoning Teach.pdf:/Users/sambarnett/Zotero/storage/V9EPJFU7/Shafto et al. - 2014 - A rational account of pedagogical reasoning Teach.pdf:application/pdf}
}

@online{noauthor_probabilistic_nodate,
	title = {Probabilistic language understanding},
	url = {https://www.problang.org/},
	urldate = {2019-10-14},
	file = {Probabilistic language understanding:/Users/sambarnett/Zotero/storage/785ZLAUA/www.problang.org.html:text/html}
}

@article{goodman_pragmatic_2016,
	title = {Pragmatic language interpretation as probabilistic inference},
	volume = {20},
	abstract = {Understanding language is more than use of ﬁxed conventions and more than decoding combinatorial structure. Instead, comprehenders make exquisitely sensitive inferences about what utterances mean given their knowledge of the speaker, the language, and the context. Building on developments in game theory and probabilistic modeling, we describe the rational speech act ({RSA}) model of pragmatic reasoning. {RSA} and its extensions provide a principled way to formalize inferences about meaning in context. {RSA} models have been used to make successful quantitative predictions about human behavior in a wide variety of diﬀerent tasks and situations, and they explain how complex phenomena like hyperbole and vagueness can arise. More generally, they provide a computational framework for integrating linguistic structure, world knowledge, and context in pragmatic language understanding.},
	pages = {818--829},
	number = {11},
	journaltitle = {Trends in cognitive sciences},
	author = {Goodman, Noah D and Frank, Michael C},
	date = {2016},
	langid = {english},
	file = {Goodman and Frank - Pragmatic language interpretation as probabilistic.pdf:/Users/sambarnett/Zotero/storage/RY2HBT6B/Goodman and Frank - Pragmatic language interpretation as probabilistic.pdf:application/pdf}
}

@article{mckenzie_when_2002,
	title = {When Negative Evidence Increases Confidence: Change in Belief After Hearing Two Sides of a Dispute},
	volume = {15},
	abstract = {Four experiments examined change in confidence after hearing two sides of a dispute. The results showed that a case independently judged to weakly support one side often increased confidence that the opposing side was correct. Furthermore, the stronger the first case, the more likely a subsequent weak case had a reverse impact. Traditional belief-updating models, which tend to focus on change in belief after individual pieces of evidence rather than entire cases, cannot account for these results, and a model that can account for them is introduced. In the new model, case strength is evaluated with respect to a relatively demanding (and malleable) reference point. A weak case can fall below this demanding reference point, resulting in a reverse impact on confidence. Cases must exceed relatively high strength thresholds in order to have their intended impact because they are expected to be biased summaries of evidence. When it is clear that a weak case is unbiased, it affects confidence in the intended direction. Copyright \# 2002 John Wiley \& Sons, Ltd.},
	pages = {17},
	number = {1},
	journaltitle = {Journal of Behavioral Decision Making},
	author = {{McKenzie}, Craig R M and Lee, Susanna M and Chen, Karen K},
	date = {2002},
	langid = {english},
	file = {McKENZIE et al. - 2002 - When Negative Evidence Increases Con®dence Change.pdf:/Users/sambarnett/Zotero/storage/IIRIAGJN/McKENZIE et al. - 2002 - When Negative Evidence Increases Con®dence Change.pdf:application/pdf}
}

@article{fernbach_when_2011,
	title = {When good evidence goes bad: The weak evidence effect in judgment and decision-making},
	volume = {119},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027711000394},
	doi = {10.1016/j.cognition.2011.01.013},
	shorttitle = {When good evidence goes bad},
	abstract = {An indispensable principle of rational thought is that positive evidence should increase belief. In this paper, we demonstrate that people routinely violate this principle when predicting an outcome from a weak cause. In Experiment 1 participants given weak positive evidence judged outcomes of public policy initiatives to be less likely than participants given no evidence, even though the evidence was separately judged to be supportive. Experiment 2 ruled out a pragmatic explanation of the result, that the weak evidence implies the absence of stronger evidence. In Experiment 3, weak positive evidence made people less likely to gamble on the outcome of the 2010 United States mid-term Congressional election. Experiments 4 and 5 replicated these ﬁndings with everyday causal scenarios. We argue that this ‘‘weak evidence effect’’ arises because people focus disproportionately on the mentioned weak cause and fail to think about alternative causes.},
	pages = {459--467},
	number = {3},
	journaltitle = {Cognition},
	shortjournal = {Cognition},
	author = {Fernbach, Philip M. and Darlow, Adam and Sloman, Steven A.},
	urldate = {2019-10-15},
	date = {2011-06},
	langid = {english},
	file = {Fernbach et al. - 2011 - When good evidence goes bad The weak evidence eff.pdf:/Users/sambarnett/Zotero/storage/2BE5TAGX/Fernbach et al. - 2011 - When good evidence goes bad The weak evidence eff.pdf:application/pdf}
}

@article{falk_persuasion_2018,
	title = {Persuasion, Influence, and Value: Perspectives from Communication and Social Neuroscience},
	volume = {69},
	issn = {0066-4308, 1545-2085},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-122216-011821},
	doi = {10.1146/annurev-psych-122216-011821},
	shorttitle = {Persuasion, Influence, and Value},
	abstract = {Opportunities to persuade and be persuaded are ubiquitous. What determines whether inﬂuence spreads and takes hold? This review provides an overview of evidence for the central role of subjective valuation in persuasion and social inﬂuence for both propagators and receivers of inﬂuence. We ﬁrst review evidence that decisions to communicate information are determined by the subjective value a communicator expects to gain from sharing. We next review evidence that the effects of social inﬂuence and persuasion on receivers, in turn, arise from changes in the receiver’s subjective valuation of objects, ideas, and behaviors. We then review evidence that self-related and social considerations are two key inputs to the value calculation in both communicators and receivers. Finally, we highlight biological coupling between communicators and receivers as a mechanism through which perceptions of value can be transmitted.},
	pages = {329--356},
	number = {1},
	journaltitle = {Annual Review of Psychology},
	shortjournal = {Annu. Rev. Psychol.},
	author = {Falk, Emily and Scholz, Christin},
	urldate = {2019-10-17},
	date = {2018-01-04},
	langid = {english},
	file = {Falk and Scholz - 2018 - Persuasion, Influence, and Value Perspectives fro.pdf:/Users/sambarnett/Zotero/storage/CF58TANK/Falk and Scholz - 2018 - Persuasion, Influence, and Value Perspectives fro.pdf:application/pdf}
}

@article{lewis_deal_2017,
	title = {Deal or No Deal? End-to-End Learning for Negotiation Dialogues},
	url = {http://arxiv.org/abs/1706.05125},
	shorttitle = {Deal or No Deal?},
	abstract = {Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for {AI}. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).},
	journaltitle = {{arXiv}:1706.05125 [cs]},
	author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann N. and Parikh, Devi and Batra, Dhruv},
	urldate = {2019-10-17},
	date = {2017-06-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.05125},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Lewis et al. - 2017 - Deal or No Deal End-to-End Learning for Negotiati.pdf:/Users/sambarnett/Zotero/storage/ZQBDAUQM/Lewis et al. - 2017 - Deal or No Deal End-to-End Learning for Negotiati.pdf:application/pdf}
}

@article{hawthorne-madell_so_2017,
	title = {So Good It Has to Be True: Wishful Thinking in Theory of Mind},
	volume = {1},
	issn = {2470-2986},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/OPMI_a_00011},
	doi = {10.1162/OPMI_a_00011},
	shorttitle = {So Good It Has to Be True},
	abstract = {In standard decision theory, rational agents are objective, keeping their beliefs independent from their desires. Such agents are the basis for current computational models of Theory of Mind ({ToM}), but the accuracy of these models are unknown. Do people really think that others do not let their desires color their beliefs? In two experiments we test whether people think that others engage in wishful thinking. We ﬁnd that participants do think others believe that desirable events are more likely to happen, and that undesirable ones are less likely to happen. However, these beliefs are not well calibrated as people do not let their desires inﬂuence their beliefs in the task. Whether accurate or not, thinking that others wishfully think has consequences for reasoning about them. We ﬁnd one such consequence—people learn more from an informant who thinks an event will happen despite wishing it was otherwise. People’s {ToM} therefore appears to be more nuanced than the current rational accounts in that it allows other’s desires to directly affect their subjective probability of an event.},
	pages = {101--110},
	number = {2},
	journaltitle = {Open Mind},
	shortjournal = {Open Mind},
	author = {Hawthorne-Madell, Daniel and Goodman, Noah D.},
	urldate = {2019-10-17},
	date = {2017-09},
	langid = {english},
	file = {Hawthorne-Madell and Goodman - 2017 - So Good It Has to Be True Wishful Thinking in The.pdf:/Users/sambarnett/Zotero/storage/QGRZPNR3/Hawthorne-Madell and Goodman - 2017 - So Good It Has to Be True Wishful Thinking in The.pdf:application/pdf}
}

@report{oey_designing_2019,
	title = {Designing good deception: Recursive theory of mind in lying and lie detection},
	url = {https://osf.io/5s4wc},
	shorttitle = {Designing good deception},
	abstract = {The human ability to deceive others and detect deception has long been tied to theory of mind. We make a stronger argument: in order to be adept liars – to balance gain (i.e. maximizing their own reward) and plausibility (i.e. maintaining a realistic lie) – humans calibrate their lies under the assumption that their partner is a rational, utility-maximizing agent. We develop an adversarial recursive Bayesian model that aims to formalize the behaviors of liars and lie detectors. We compare this model to (1) a model that does not perform theory of mind computations and (2) a model that has perfect knowledge of the opponent’s behavior. To test these models, we introduce a novel dyadic, stochastic game, allowing for quantitative measures of lies and lie detection. In a second experiment, we vary the ground truth probability. We ﬁnd that our rational models qualitatively predict human lying and lie detecting behavior better than the non-rational model. Our ﬁndings suggest that humans control for the extremeness of their lies in a manner reﬂective of rational social inference. These ﬁndings provide a new paradigm and formal framework for nuanced quantitative analysis of the role of rationality and theory of mind in lying and lie detecting behavior.},
	institution = {{PsyArXiv}},
	type = {preprint},
	author = {Oey, Lauren A. and Schachner, Adena and Vul, Edward},
	urldate = {2019-10-25},
	date = {2019-05-15},
	langid = {english},
	doi = {10.31234/osf.io/5s4wc},
	file = {Oey et al. - 2019 - Designing good deception Recursive theory of mind.pdf:/Users/sambarnett/Zotero/storage/84R6GJ65/Oey et al. - 2019 - Designing good deception Recursive theory of mind.pdf:application/pdf}
}

@article{perfors_stronger_2018,
	title = {Stronger evidence isn’t always better: A role for social inference in evidence selection and interpretation},
	abstract = {Much of what we know comes from other people, and the quantity of information provided is often constrained by time or space. For a communicator, what information they choose to convey depends not just on the nature of their topic, but also on the social inferences their listeners will make about them based on what they say. For the listener, their interpretation of information given to them depends not just on the information itself, but also on what inferences they make about the bias and motivations of the communicator they received it from. In this paper we explore how and whether these social factors interact with the “true” nature of the information being communicated. We ﬁnd that stronger evidence does not always lead to stronger conclusions and often leads to increased perceived bias. Communicators, perhaps for this reason and perhaps for others, often modulate the evidence they present to be less unanimous than warranted. This has implications for real-world situations, like communicating about climate change: in such situations, both communicators and listeners behave in what may be individually rational ways, but the end result is that the underlying truth gets distorted.},
	pages = {6},
	journaltitle = {{CogSci}},
	author = {Perfors, Amy and Navarro, Daniel J and Shafto, Patrick},
	date = {2018},
	langid = {english},
	file = {Perfors et al. - Stronger evidence isn’t always better A role for .pdf:/Users/sambarnett/Zotero/storage/T5USHKT3/Perfors et al. - Stronger evidence isn’t always better A role for .pdf:application/pdf}
}

@article{serrino_finding_2019,
	title = {Finding Friend and Foe in Multi-Agent Games},
	url = {http://arxiv.org/abs/1906.02330},
	abstract = {Recent breakthroughs in {AI} for multi-agent games like Go, Poker, and Dota, have seen great strides in recent years. Yet none of these games address the real-life challenge of cooperation in the presence of unknown and uncertain teammates. This challenge is a key game mechanism in hidden role games. Here we develop the {DeepRole} algorithm, a multi-agent reinforcement learning agent that we test on The Resistance: Avalon, the most popular hidden role game. {DeepRole} combines counterfactual regret minimization ({CFR}) with deep value networks trained through self-play. Our algorithm integrates deductive reasoning into vector-form {CFR} to reason about joint beliefs and deduce partially observable actions. We augment deep value networks with constraints that yield interpretable representations of win probabilities. These innovations enable {DeepRole} to scale to the full Avalon game. Empirical game-theoretic methods show that {DeepRole} outperforms other hand-crafted and learned agents in ﬁve-player Avalon. {DeepRole} played with and against human players on the web in hybrid human-agent teams. We ﬁnd that {DeepRole} outperforms human players as both a cooperator and a competitor.},
	journaltitle = {{arXiv}:1906.02330 [cs, stat]},
	author = {Serrino, Jack and Kleiman-Weiner, Max and Parkes, David C. and Tenenbaum, Joshua B.},
	urldate = {2019-11-05},
	date = {2019-06-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.02330},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Multiagent Systems},
	file = {Serrino et al. - 2019 - Finding Friend and Foe in Multi-Agent Games.pdf:/Users/sambarnett/Zotero/storage/FHHFPXCZ/Serrino et al. - 2019 - Finding Friend and Foe in Multi-Agent Games.pdf:application/pdf}
}

@article{perez_finding_2019,
	title = {Finding Generalizable Evidence by Learning to Convince Q\&A Models},
	url = {http://arxiv.org/abs/1909.05863},
	abstract = {We propose a system that ﬁnds the strongest supporting evidence for a given answer to a question, using passage-based questionanswering ({QA}) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained {QA} model of a given answer, if the {QA} model received those sentences instead of the full passage. Rather than ﬁnding evidence that convinces one model alone, we ﬁnd that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other {QA} models and humans. Given its general nature, this approach improves {QA} in a robust manner: using agentselected evidence (i) humans can correctly answer questions with only ∼20\% of the full passage and (ii) {QA} models can generalize to longer passages and harder questions.},
	journaltitle = {{arXiv}:1909.05863 [cs]},
	author = {Perez, Ethan and Karamcheti, Siddharth and Fergus, Rob and Weston, Jason and Kiela, Douwe and Cho, Kyunghyun},
	urldate = {2019-11-05},
	date = {2019-09-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.05863},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Perez et al. - 2019 - Finding Generalizable Evidence by Learning to Conv.pdf:/Users/sambarnett/Zotero/storage/LTEMDES9/Perez et al. - 2019 - Finding Generalizable Evidence by Learning to Conv.pdf:application/pdf}
}

@article{chen_cicero:_2018,
	title = {Cicero: Multi-Turn, Contextual Argumentation for Accurate Crowdsourcing},
	url = {http://arxiv.org/abs/1810.10733},
	shorttitle = {Cicero},
	abstract = {Traditional approaches for ensuring high quality crowdwork have failed to achieve high-accuracy on difﬁcult problems. Aggregating redundant answers often fails on the hardest problems when the majority is confused. Argumentation has been shown to be effective in mitigating these drawbacks. However, existing argumentation systems only support limited interactions and show workers general justiﬁcations, not contextspeciﬁc arguments targeted to their reasoning.},
	journaltitle = {{arXiv}:1810.10733 [cs]},
	author = {Chen, Quanze and Bragg, Jonathan and Chilton, Lydia B. and Weld, Daniel S.},
	urldate = {2019-11-05},
	date = {2018-10-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.10733},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {Chen et al. - 2018 - Cicero Multi-Turn, Contextual Argumentation for A.pdf:/Users/sambarnett/Zotero/storage/7N2HDTLB/Chen et al. - 2018 - Cicero Multi-Turn, Contextual Argumentation for A.pdf:application/pdf}
}

@article{irving_ai_2018,
	title = {{AI} safety via debate},
	url = {http://arxiv.org/abs/1805.00899},
	abstract = {To make {AI} systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in {PSPACE} given polynomial time judges (direct judging answers only {NP} questions). In practice, whether debate works involves empirical questions about humans and the tasks we want {AIs} to perform, plus theoretical questions about the meaning of {AI} alignment. We report results on an initial {MNIST} experiment where agents compete to convince a sparse classiﬁer, boosting the classiﬁer’s accuracy from 59.4\% to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.},
	journaltitle = {{arXiv}:1805.00899 [cs, stat]},
	author = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
	urldate = {2019-05-08},
	date = {2018-05-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.00899},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{harris_james_2013,
	title = {James is polite and punctual (and useless): A Bayesian formalisation of faint praise},
	volume = {19},
	issn = {1354-6783, 1464-0708},
	url = {https://www.tandfonline.com/doi/full/10.1080/13546783.2013.801367},
	doi = {10.1080/13546783.2013.801367},
	shorttitle = {James is polite and punctual (and useless)},
	pages = {414--429},
	number = {3},
	journaltitle = {Thinking \& Reasoning},
	shortjournal = {Thinking \& Reasoning},
	author = {Harris, Adam J. L. and Corner, Adam and Hahn, Ulrike},
	urldate = {2019-11-12},
	date = {2013-09},
	langid = {english},
	file = {Harris et al. - 2013 - James is polite and punctual (and useless) A Baye.pdf:/Users/sambarnett/Zotero/storage/JSPITA3L/Harris et al. - 2013 - James is polite and punctual (and useless) A Baye.pdf:application/pdf}
}

@article{grice_logic_1975,
	title = {Logic and conversation},
	pages = {41--58},
	journaltitle = {1975},
	author = {Grice, H. Paul and Cole, Peter and Morgan, Jerry},
	date = {1975},
	file = {Full Text:/Users/sambarnett/Zotero/storage/B4HW8FKF/Grice et al. - 1975 - Logic and conversation.pdf:application/pdf;Snapshot:/Users/sambarnett/Zotero/storage/SIV233AD/books.html:text/html}
}